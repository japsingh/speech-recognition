{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6835,)\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "Execution time in seconds: 38.64708995819092\n"
     ]
    }
   ],
   "source": [
    "data_path = r\"E:\\personal\\LiveProjects\\speech_recognition\\google_speech_new\\google_speech\\test\"\n",
    "csv_name = r\"test.csv\"\n",
    "#data_path_train = r\"E:\\personal\\LiveProjects\\speech_recognition\\google_speech_new\\google_speech\\train\"\n",
    "#csv_name_train = r\"train.csv\"\n",
    "\n",
    "train_csv = os.path.join(data_path, csv_name)\n",
    "df = pd.read_csv(train_csv, encoding='utf-8')\n",
    "#print(df)\n",
    "# Function to return Mel spectogram of given audio file\n",
    "def get_mel_spectogram(data, samplerate):\n",
    "    return librosa.feature.melspectrogram(y=data, sr=samplerate)\n",
    "\n",
    "def get_mel_spectogram_from_audio_path(audio_path):\n",
    "    data, samplerate = librosa.load(audio_path)\n",
    "    M = get_mel_spectogram(data, samplerate)\n",
    "    return M\n",
    "\n",
    "# Generate mel spectogram for all training audio files\n",
    "def make_1s_duration(data, duration):\n",
    "    return librosa.effects.time_stretch(data, duration)\n",
    "\n",
    "startTime = time.time()\n",
    "x_train = np.array([])\n",
    "y_train = pd.get_dummies(df['label'])\n",
    "y_train = y_train.to_numpy()\n",
    "\n",
    "#for ind in df.index:\n",
    "#for ind in np.arange(100):\n",
    "def featurize_clip1(fp):\n",
    "    #fp = df.loc[ind, \"file_path\"]\n",
    "    audio_path = os.path.join(data_path, os.path.join(r\"audio\", fp))\n",
    "    #print(audio_path)\n",
    "    data, samplerate = librosa.load(audio_path, sr=None)\n",
    "    #print(samplerate)\n",
    "    duration = librosa.get_duration(y=data, sr=samplerate)\n",
    "    #print(duration)\n",
    "    #print(data.shape[0])\n",
    "    data_stretch = make_1s_duration(data, duration)\n",
    "    #print(data_stretch.shape)\n",
    "    M = get_mel_spectogram(data_stretch, samplerate)\n",
    "    #print(np.shape(M))\n",
    "    #if ind == 0:\n",
    "    #    x_train = M\n",
    "    #else:\n",
    "    #    x_train = np.vstack((x_train, M)).astype(np.float32)\n",
    "    #y_train[ind] = df.loc[ind, \"label\"]\n",
    "    return M\n",
    "\n",
    "train_audio_path = f\"{data_path}/audio\"\n",
    "\n",
    "def pad_audio_with_silence(audio: np.ndarray, sample_rate: int, duration: float):\n",
    "    target_n_samples = int(duration * sample_rate)\n",
    "    #assert audio.shape[0] < target_n_samples\n",
    "    padded_audio = np.zeros(target_n_samples)\n",
    "    padded_audio[:audio.shape[0]] = audio\n",
    "    return padded_audio\n",
    "\n",
    "def mel_spectrogram(\n",
    "    audio: np.ndarray,\n",
    "    sample_rate: int,\n",
    "    threshold: int=None,\n",
    "):\n",
    "    melspec = librosa.feature.melspectrogram(y=audio, sr=sample_rate)\n",
    "    melspec = librosa.power_to_db(melspec, ref=np.max)\n",
    "    if threshold:\n",
    "        melspec[melspec < threshold] = -80\n",
    "    return melspec\n",
    "\n",
    "def featurize_clip(file_path):\n",
    "    audio, sample_rate = librosa.load(f\"{train_audio_path}/{file_path}\", sr=None)\n",
    "    audio = pad_audio_with_silence(audio, sample_rate, duration=1)\n",
    "    spectrogram = mel_spectrogram(audio, sample_rate)\n",
    "    return spectrogram\n",
    "\n",
    "df[\"spectrogram\"] = df[\"file_path\"].apply(featurize_clip)\n",
    "\n",
    "print(np.shape(df[\"spectrogram\"]))\n",
    "#x_train = x_train.reshape((-1, 64, 64, 1))\n",
    "\n",
    "\n",
    "print(y_train)\n",
    "#y_train = to_categorical(y_train)\n",
    "#print(np.shape(y_train))\n",
    "\n",
    "executionTime = (time.time() - startTime)\n",
    "print('Execution time in seconds: ' + str(executionTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "#from tensorflow.keras.layers import Rescaling\n",
    "#from tensorflow.keras.layers.experimental.preprocessing import Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 128, 32, 1)]      0         \n",
      "                                                                 \n",
      " rescaling_7 (Rescaling)     (None, 128, 32, 1)        0         \n",
      "                                                                 \n",
      " conv2d_25 (Conv2D)          (None, 126, 30, 32)       320       \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 63, 15, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_26 (Conv2D)          (None, 61, 13, 64)        18496     \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 50752)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 30)                1522590   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,541,406\n",
      "Trainable params: 1,541,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_classes = df[\"label\"].unique().shape[0]\n",
    "n_classes\n",
    "\n",
    "inputs = keras.Input(shape=(128, 32, 1))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "#x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "#x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "#x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "#x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "#x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "#x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(units=n_classes, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6835, 128, 32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# network parameters\n",
    "#input_shape = (64, 64, 1)\n",
    "batch_size = 128\n",
    "kernel_size = 3\n",
    "filters = 64\n",
    "dropout = 0.3\n",
    "\n",
    "spectrogram_shape = df[\"spectrogram\"].iloc[1].shape\n",
    "spectrogram_shape\n",
    "\n",
    "input_shape = (spectrogram_shape[0], spectrogram_shape[1], 1)\n",
    "input_shape\n",
    "\n",
    "X = np.stack(df[\"spectrogram\"])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = df[\"spectrogram\"].to_numpy()\n",
    "\n",
    "#print(x_train[0].shape)\n",
    "\n",
    "#def stack_it(arr):\n",
    "#    return arr.shape()\n",
    "#df[\"spectrogram_processed\"] = df[\"spectrogram\"].apply(stack_it)\n",
    "import tensorflow as tf\n",
    "class DataframeSequence(keras.utils.Sequence):\n",
    "    def __init__(self, df, y_train, batch_size):\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.df) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.df['spectrogram'][idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y_train[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        print(batch_x.shape)\n",
    "        return tf.convert_to_tensor(batch_x), tf.convert_to_tensor(batch_y)\n",
    "\n",
    "sequence = DataframeSequence(df, y_train, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n",
      "Epoch 1/20\n",
      "54/54 [==============================] - 32s 585ms/step - loss: 3.2523 - accuracy: 0.0969\n",
      "Epoch 2/20\n",
      "54/54 [==============================] - 35s 651ms/step - loss: 2.3202 - accuracy: 0.3606\n",
      "Epoch 3/20\n",
      "54/54 [==============================] - 34s 635ms/step - loss: 1.8607 - accuracy: 0.4808\n",
      "Epoch 4/20\n",
      "54/54 [==============================] - 40s 746ms/step - loss: 1.6531 - accuracy: 0.5388\n",
      "Epoch 5/20\n",
      "54/54 [==============================] - 41s 758ms/step - loss: 1.4854 - accuracy: 0.5899\n",
      "Epoch 6/20\n",
      "54/54 [==============================] - 44s 812ms/step - loss: 1.3541 - accuracy: 0.6209\n",
      "Epoch 7/20\n",
      "54/54 [==============================] - 42s 779ms/step - loss: 1.2247 - accuracy: 0.6631\n",
      "Epoch 8/20\n",
      "54/54 [==============================] - 42s 783ms/step - loss: 1.0900 - accuracy: 0.6938\n",
      "Epoch 9/20\n",
      "54/54 [==============================] - 43s 804ms/step - loss: 0.9692 - accuracy: 0.7276\n",
      "Epoch 10/20\n",
      "54/54 [==============================] - 42s 772ms/step - loss: 0.8411 - accuracy: 0.7636\n",
      "Epoch 11/20\n",
      "54/54 [==============================] - 44s 812ms/step - loss: 0.7574 - accuracy: 0.7890\n",
      "Epoch 12/20\n",
      "54/54 [==============================] - 43s 797ms/step - loss: 0.6417 - accuracy: 0.8231\n",
      "Epoch 13/20\n",
      "54/54 [==============================] - 42s 776ms/step - loss: 0.5464 - accuracy: 0.8467\n",
      "Epoch 14/20\n",
      "54/54 [==============================] - 43s 789ms/step - loss: 0.4665 - accuracy: 0.8729\n",
      "Epoch 15/20\n",
      "54/54 [==============================] - 43s 803ms/step - loss: 0.4022 - accuracy: 0.8895\n",
      "Epoch 16/20\n",
      "54/54 [==============================] - 43s 805ms/step - loss: 0.3410 - accuracy: 0.9110\n",
      "Epoch 17/20\n",
      "54/54 [==============================] - 42s 777ms/step - loss: 0.2950 - accuracy: 0.9222\n",
      "Epoch 18/20\n",
      "54/54 [==============================] - 43s 803ms/step - loss: 0.2412 - accuracy: 0.9410\n",
      "Epoch 19/20\n",
      "54/54 [==============================] - 43s 802ms/step - loss: 0.1961 - accuracy: 0.9544\n",
      "Epoch 20/20\n",
      "54/54 [==============================] - 43s 808ms/step - loss: 0.1550 - accuracy: 0.9688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x221571a3550>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model with input images and labels\n",
    "print(x_train.shape)\n",
    "#model.fit_generator(sequence, epochs=20)\n",
    "model.fit(X,\n",
    "          y_train,\n",
    "          #validation_data=(x_test, y_test),\n",
    "          #sequence,\n",
    "          epochs=20,\n",
    "          batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
